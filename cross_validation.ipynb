{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Train Split and Cross Validation\n",
    "\n",
    "## Review of train/test validation methods\n",
    "\n",
    "We've discussed overfitting, underfitting, and how to validate the \"generalizeability\" of your models by testing them on unseen data. \n",
    "\n",
    "In this lab you'll practice two related validation methods: \n",
    "1. **train/test split**\n",
    "2. **k-fold cross-validation**\n",
    "\n",
    "Train/test split and k-fold cross-validation both serve two useful purposes:\n",
    "- We prevent overfitting by not using all the data, and\n",
    "- We retain some remaining data to evaluate our model.\n",
    "\n",
    "In the case of cross-validation, the model fitting and evaluation is performed multiple times on different train/test splits of the data.\n",
    "\n",
    "Ultimately we can the training and testing validation framework to compare multiple models on the same dataset. This could be comparisons of two linear models or of completely different models on the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "For your independent practice, fit **three different models** on the Boston housing data. For example, you could pick three different subsets of variables, one or more polynomial models, or any other model that you like. \n",
    "\n",
    "**Start with train/test split validation:**\n",
    "* Fix a testing/training split of the data\n",
    "* Train each of your models on the training data\n",
    "* Evaluate each of the models on the test data\n",
    "* Rank the models by how well they score on the testing data set.\n",
    "\n",
    "**Then try K-Fold cross-validation:**\n",
    "* Perform a k-fold cross validation and use the cross-validation scores to compare your models. Did this change your rankings?\n",
    "* Try a few different K-splits of the data for the same models.\n",
    "\n",
    "If you're interested, try a variety of response variables.  We start with **MEDV** (the `.target` attribute from the dataset load method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean up any data problems\n",
    "\n",
    "Load the Boston housing data.  Fix any problems, if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Select 3-4 variables with your dataset to perform a 50/50 test train split on\n",
    "\n",
    "- Train your model on the the **training** data\n",
    "- Score and plot your predictions for the **test** data (i.e., a scatter plot of actual y values versus predicted y values)\n",
    "- How well did your model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Try ratios of 70/30 and 90/10 for the training set to test set\n",
    "\n",
    "- Score and plot your test set (i.e., a scatter plot of actual y values versus predicted y values)\n",
    "- How does the score change across both of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Try K-Folds cross-validation with _k_ between 5-10 for your regression. \n",
    "\n",
    "You can use either `cross_val_score` / `cross_val_predict` or the `KFolds` iterator\n",
    "\n",
    "- What set of variables are optimal? \n",
    "- How do your scores change?  \n",
    "- How does the variance of the scores change?\n",
    "- Try different folds to get a sense of how this impacts your score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. [Bonus] optimize the $R^2$ score\n",
    "\n",
    "Can you optimize your R^2 by selecting the best features and validating the model using train/test split and K-Folds? Set up a scheme to do the following:\n",
    "\n",
    "1. Create a holdout test set using `train_test_split()`. \n",
    "  - We will use the training set in our cross-validation part below to find the right set of columns.\n",
    "  - We will check the best set of columns that we have found against the holdout test as a final check on our work\n",
    "2. Iterate through every pair of columns (use a for loop!) in the training set you created in step 1 and look at the average $R^2$ score found by `cross_val_score()`\n",
    "3. Pick the pair of columns that has the highest cross-validated $R^2$ code and fit a Linear Regression using those columns as the predictors in the full training set you created in step 1.\n",
    "4. Test this \"final\" model against the holdout test set you created in step 1. How well does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Can you explain what could be wrong with this approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. [Bonus] Explore another target variable \n",
    "\n",
    "Can you find another response variable, given a combination of predictors, that can be predicted accurately through the exploration of different predictors in this dataset?\n",
    "\n",
    "> *Tip: Check out pairplots, coefficients, and pearson scores.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
